# AI Fact-Checking System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An advanced multi-agent fact-checking system built with LangGraph that automatically verifies the factual accuracy of text, video, and multilingual content. The system extracts individual claims, verifies them against real-world evidence, and generates professional Excel reports with cost tracking.

## ğŸ—ï¸ System Architecture

The system is built on a modular, multi-agent architecture with three specialized agents orchestrated by LangGraph:

1.  **[Claim Extractor (`claim_extractor/`)](./apps/agent/claim_extractor/README.md)**: Extracts factual claims using the research-based Claimify methodology
2.  **[Claim Verifier (`claim_verifier/`)](./apps/agent/claim_verifier/README.md)**: Verifies each claim against online evidence via Tavily Search API
3.  **[Fact Checker (`fact_checker/`)](./apps/agent/fact_checker/README.md)**: Orchestrates the complete pipeline with parallel processing

### Three-Agent System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER                              â”‚
â”‚  â€¢ Text Input (Direct/Translated)                           â”‚
â”‚  â€¢ Video Input â†’ Whisper Transcription â†’ Translation        â”‚
â”‚  â€¢ Single Claim (Quick Check Mode)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               AGENT 1: CLAIM EXTRACTOR                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Stage 1: Sentence Splitting         â”‚                  â”‚
â”‚  â”‚  â€¢ Identify fact-bearing sentences   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Stage 2: Selection                  â”‚                  â”‚
â”‚  â”‚  â€¢ Filter verifiable statements      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Stage 3: Disambiguation             â”‚                  â”‚
â”‚  â”‚  â€¢ Resolve pronouns & references     â”‚                  â”‚
â”‚  â”‚  â€¢ Multi-LLM consensus voting        â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Stage 4: Decomposition              â”‚                  â”‚
â”‚  â”‚  â€¢ Break complex â†’ atomic claims     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Stage 5: Validation                 â”‚                  â”‚
â”‚  â”‚  â€¢ Final checkability verification   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               AGENT 2: CLAIM VERIFIER                       â”‚
â”‚          (Runs in parallel for each claim)                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Node 1: Generate Search Query       â”‚                  â”‚
â”‚  â”‚  â€¢ Extract keywords from claim       â”‚                  â”‚
â”‚  â”‚  â€¢ Optimize for web search           â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Node 2: Retrieve Evidence           â”‚                  â”‚
â”‚  â”‚  â€¢ Tavily Search API (max 10/call)   â”‚                  â”‚
â”‚  â”‚  â€¢ Aggregate snippets + sources      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                     â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Node 3: Search Decision             â”‚                  â”‚
â”‚  â”‚  â€¢ Check authoritative sources       â”‚                  â”‚
â”‚  â”‚  â€¢ Evaluate evidence sufficiency     â”‚                  â”‚
â”‚  â”‚  â€¢ Decide: CONTINUE or STOP          â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚           â†“                    â†“                            â”‚
â”‚      [Loop Back]          [Proceed]                         â”‚
â”‚           â”‚                    â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚  â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Node 4: Evaluate Evidence           â”‚                  â”‚
â”‚  â”‚  â€¢ Analyze all gathered evidence     â”‚                  â”‚
â”‚  â”‚  â€¢ Generate verdict:                 â”‚                  â”‚
â”‚  â”‚    - SUPPORTED                       â”‚                  â”‚
â”‚  â”‚    - REFUTED                         â”‚                  â”‚
â”‚  â”‚    - NOT_ENOUGH_INFO                 â”‚                  â”‚
â”‚  â”‚    - CONFLICTING_EVIDENCE            â”‚                  â”‚
â”‚  â”‚  â€¢ Provide detailed reasoning        â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               AGENT 3: FACT CHECKER                         â”‚
â”‚             (Orchestrator & Aggregator)                     â”‚
â”‚                                                             â”‚
â”‚  â€¢ Manages overall workflow                                 â”‚
â”‚  â€¢ Coordinates parallel claim verification                  â”‚
â”‚  â€¢ Rate limiting (max 4 concurrent)                         â”‚
â”‚  â€¢ Aggregates all results                                   â”‚
â”‚  â€¢ Tracks costs & metrics                                   â”‚
â”‚  â€¢ Generates final report                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OUTPUT LAYER                             â”‚
â”‚  â€¢ Professional Excel Report (4 sheets)                     â”‚
â”‚  â€¢ Real-time metrics dashboard                              â”‚
â”‚  â€¢ Downloadable results                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11 or higher
- OpenAI API key
- Tavily API key (1,000 free searches/month)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/bharathxd/ClaimAI.git
cd ClaimAI
```

2. **Install dependencies**
```bash
cd apps/streamlit
pip install -r requirements.txt
```

3. **Set up API keys**

Create a `.streamlit/secrets.toml` file:
```toml
OPENAI_API_KEY = "your-openai-api-key"
TAVILY_API_KEY = "your-tavily-api-key"
```

Or set environment variables:
```bash
export OPENAI_API_KEY="your-openai-api-key"
export TAVILY_API_KEY="your-tavily-api-key"
```

4. **Launch the application**
```bash
streamlit run standalone_app.py
```

The app will open in your browser at `http://localhost:8501`

### Basic Usage

#### Mode 1: Full Text Analysis
1. Select "ğŸ“„ Full Text Analysis" from sidebar
2. Enter or paste text to analyze
3. Click "ğŸ” Start Fact-Checking"
4. Review extracted claims and verification results
5. Download Excel report

#### Mode 2: Single Fact Verification
1. Select "âœ“ Single Fact Verification"
2. Enter a single claim to verify
3. Click "ğŸ” Verify This Fact"
4. Get instant verdict with evidence

#### Mode 3: Claim Extraction
1. Select "ğŸ“‹ Claim Extraction Only"
2. Input text for analysis
3. Click "ğŸ” Extract Claims"
4. Review list of extracted claims
5. Optionally select claims to verify

#### Mode 4: Video Fact-Checking
1. Select "ğŸ¥ Video Fact-Checking"
2. Upload video file (MP4/AVI/MOV)
3. System automatically:
   - Extracts audio
   - Transcribes speech (Whisper)
   - Detects and translates language
   - Extracts and verifies claims
4. Download comprehensive Excel report

## ğŸ“– Usage Examples

### Example 1: Verifying a News Article

**Input Text:**
```
According to a White House statement, the US economy added 250,000 
jobs in January 2024. Tesla delivered over 1.8 million vehicles in 
2023, making it the world's best-selling EV manufacturer. The company's 
CEO stated they aim to produce 20 million cars annually by 2030.
```

**Output:**
```
âœ“ SUPPORTED: "Tesla delivered over 1.8 million vehicles in 2023"
  Evidence: Official Tesla Q4 2023 earnings report
  
âœ— REFUTED: "The US economy added 250,000 jobs in January 2024"
  Evidence: Bureau of Labor Statistics reports 353,000 jobs added
  
âœ“ SUPPORTED: "Tesla is the world's best-selling EV manufacturer in 2023"
  Evidence: Multiple automotive industry reports
  
âš ï¸ NOT ENOUGH INFO: "Tesla aims to produce 20 million cars annually by 2030"
  Evidence: Various statements, but no official confirmation of 20M target
```

**Metrics:**
- 4 claims extracted
- 3 verified successfully
- Cost: â‚¹7.60 total
- Time: ~45 seconds

### Example 2: Quick Fact Check

**Mode:** Single Fact Verification

**Input:** "The Eiffel Tower was completed in 1889"

**Output:**
```
âœ“ SUPPORTED

Reasoning: Multiple authoritative sources including official Eiffel Tower 
website and historical records confirm completion in 1889 for the World's 
Fair in Paris.

Sources:
- https://www.toureiffel.paris/en/the-monument/history
- https://www.britannica.com/topic/Eiffel-Tower-Paris-France
```

**Metrics:**
- 1 claim verified
- Cost: â‚¹1.90
- Time: ~8 seconds

### Example 3: Video Fact-Checking

**Input:** Political speech video (5 minutes, MP4)

**Process:**
1. Audio extracted â†’ 4.8 MB WAV
2. Transcribed with Whisper â†’ 1,247 words
3. Language detected â†’ English
4. 12 claims extracted
5. 12 claims verified in parallel

**Sample Results:**
```
âœ“ SUPPORTED: "Unemployment is at a 50-year low" (8 claims supported)
âš ï¸ NOT ENOUGH INFO: "Our economy is the strongest in the world" (2 claims insufficient)
âœ— REFUTED: "We've created more jobs than any previous administration" (2 claims refuted)
```

**Final Report:**
- Accuracy: 66.7% (8/12 supported)
- Total cost: â‚¹22.80
- Processing time: 2 min 15 sec
- Excel report: 4 sheets, 38 rows of evidence

### Example 4: Multi-Language Content

**Input:** Spanish article about climate change

**Process:**
1. Language auto-detected â†’ Spanish
2. Auto-translated to English for processing
3. 7 claims extracted
4. Verified against English sources

**Output:**
All claims verified with proper context preservation. Original Spanish 
claims shown in Excel report alongside English translations.

## ğŸ¯ How It Works

### Step 1: Claim Extraction
The system analyzes input text and extracts verifiable claims through a 4-stage pipeline:

- **Selection**: Identifies sentences containing factual information
- **Disambiguation**: Resolves ambiguous references and pronouns using multi-LLM consensus voting
- **Decomposition**: Breaks compound statements into atomic, independently verifiable claims
- **Validation**: Ensures each extracted claim is clear, self-contained, and checkable

**Example:**
```
Input: "He founded SpaceX in 2002. The company launched Falcon 9."

After extraction:
âœ“ "Elon Musk founded SpaceX in 2002"
âœ“ "SpaceX launched the Falcon 9 rocket"
```

### Step 2: Claim Verification
Each extracted claim is independently verified:

1. **Query Generation**: Constructs optimal search queries using LLM
2. **Evidence Retrieval**: Gathers supporting/refuting evidence via Tavily Search
3. **Iterative Search**: Continues searching until sufficient evidence or confidence threshold
4. **Smart Stopping**: Early termination when authoritative sources found (gov/edu/org domains)
5. **Verdict Generation**: Analyzes all evidence to determine:
   - **SUPPORTED**: Claim backed by reliable evidence
   - **REFUTED**: Claim contradicted by evidence
   - **NOT ENOUGH INFO**: Insufficient evidence to verify

### Step 3: Report Generation
The system produces a comprehensive report including:
- Original text and extracted claims
- Per-claim verdicts with confidence levels
- Supporting/refuting evidence with sources
- Summary statistics (accuracy rate, claim breakdown)
- Token usage and cost metrics

## âœ¨ Features Overview

### ğŸ¯ Four Operation Modes

#### 1. **Full Text Analysis** (Complete Fact-Checking)
Extract all claims from text and verify each one against online evidence. Perfect for:
- Analyzing articles, blog posts, social media content
- Validating quoted statistics and figures
- Comprehensive fact-checking of long-form content

**Workflow**: Text Input â†’ Claim Extraction â†’ Parallel Verification â†’ Excel Report

#### 2. **Single Fact Verification** (Quick Check)
Instantly verify a single statement without extraction overhead. Ideal for:
- Quick spot-checks of specific claims
- Verifying quotes or statistics in real-time
- Fast fact-checking during conversations

**Workflow**: Single Claim Input â†’ Direct Verification â†’ Instant Verdict

#### 3. **Claim Extraction Only** (Content Analysis)
Extract verifiable claims without verification - analyze what's being stated. Great for:
- Content auditing and analysis
- Identifying fact-bearing statements
- Planning verification work

**Workflow**: Text Input â†’ Claim Extraction â†’ Claim List Output

#### 4. **Video Fact-Checking** (Multimedia Analysis)
Upload video files (MP4, AVI, MOV, etc.) for automatic transcription and fact-checking. Features:
- Automatic audio extraction using MoviePy
- Speech-to-text with OpenAI Whisper
- Multi-language transcription and translation
- Full fact-checking pipeline on video content

**Workflow**: Video Upload â†’ Audio Extraction â†’ Transcription â†’ Translation (if needed) â†’ Claim Extraction â†’ Verification â†’ Excel Report

### ğŸŒ Multi-Language Support
- **Auto-Detection**: Automatically identifies input language
- **Auto-Translation**: Translates non-English content to English for processing
- **Supported Languages**: 100+ languages via OpenAI's detection
- **Preserved Context**: Maintains original meaning during translation

### ğŸ“Š Professional Excel Reports
Generate beautifully formatted reports with:
- **Summary Sheet**: Overall accuracy metrics, verdict breakdown, total costs
- **Extracted Facts Sheet**: Numbered list of all claims found
- **Fact-Check Results**: Detailed verdicts with confidence scores
- **Evidence Details**: Sources with clickable URLs
- **Visual Formatting**: Color-coded verdicts, alternating rows, professional styling

### ğŸš€ Core Capabilities
- âœ… **Multi-stage Claim Extraction** - Research-based methodology (Claimify)
- âœ… **Iterative Evidence Gathering** - Continues until confidence threshold met
- âœ… **Authoritative Source Detection** - Prioritizes .gov, .edu, .org domains
- âœ… **Parallel Processing** - Verifies multiple claims concurrently (rate-limited)
- âœ… **Real-Time Metrics** - Live tracking of API costs and token usage
- âœ… **Video Processing** - MP4/AVI/MOV support with Whisper transcription
- âœ… **Multi-Language** - Auto-detect and translate 100+ languages

### âš¡ Optimizations
- âœ… **Rate Limiting** - Semaphore-based concurrency control (max 4 concurrent)
- âœ… **Circuit Breaker** - Auto-pauses on API errors to prevent retry storms
- âœ… **Search Heuristics** - Early stopping with authoritative sources saves 30% API calls
- âœ… **Smart Retries** - 3 retries with exponential backoff, 60s timeout
- âœ… **Token Tracking** - Real-time cost monitoring per pipeline stage
- âœ… **Session State** - Preserves metrics across UI interactions

### ğŸ’° Cost Metrics Dashboard
- ğŸ“Š **Live Tracking** - Real-time OpenAI and Tavily API usage
- ğŸ“Š **Per-claim costs** - LLM tokens, search calls, estimated costs
- ğŸ“Š **Stage breakdown** - Extraction vs verification costs
- ğŸ“Š **Session totals** - Cumulative metrics for entire analysis

## ğŸ’° Cost Analysis

### Current Configuration (GPT-4o-mini + Tavily)

**Per-Claim Breakdown:**
- **LLM Costs**: â‚¹0.54 per claim
  - Claim extraction: â‚¹0.18
  - Query generation: â‚¹0.08
  - Evidence evaluation: â‚¹0.28
- **Tavily Search**: â‚¹1.36 per claim (avg 2 searches)
- **Total**: â‚¹1.90 per claim

**Real-World Scenarios:**
- **Short article** (5 claims): â‚¹9.50
- **Long article** (15 claims): â‚¹28.50
- **5-minute video** (12 claims): â‚¹22.80
- **Monthly usage** (1,000 claims): â‚¹1,220 (after free tier)

**Free Tier Coverage:**
- âœ… **OpenAI**: Pay-as-you-go (no free tier)
- âœ… **Tavily**: 1,000 free searches/month
  - Covers ~500 verified claims
  - Additional searches: $0.005 each

**Cost Optimization Tips:**
1. Use "Claim Extraction Only" mode for content analysis (no Tavily costs)
2. Enable early stopping for authoritative sources (saves ~30% Tavily calls)
3. Use Single Fact Verification for spot checks
4. Batch process multiple videos to amortize extraction costs

**Detailed Cost Documentation:**
* **[Claim Extractor Cost Analysis](./apps/agent/claim_extractor/README.md)** - Token usage breakdown
* **[Claim Verifier Cost Analysis](./apps/agent/claim_verifier/README.md)** - Search and evaluation costs
* **[Full Pipeline Analysis](./CLAIM_EXTRACTOR_COST_ANALYSIS.md)** - Comprehensive cost study

## ğŸ“š Technical Details

### Technology Stack
- **Framework**: LangGraph for multi-agent orchestration
- **LLM**: OpenAI GPT-4o-mini (configurable)
- **Search**: Tavily Search API for evidence retrieval
- **Transcription**: OpenAI Whisper for video audio
- **Frontend**: Streamlit (web) + Electron (desktop)
- **Excel**: openpyxl with advanced formatting
- **Video**: MoviePy for audio extraction
- **Language**: Python 3.11+
- **Core Libraries**:
  - `langchain` - LLM framework
  - `langchain-openai` - OpenAI integrations
  - `langgraph` - Agent workflow orchestration
  - `streamlit` - Web UI
  - `openpyxl` - Excel generation
  - `moviepy` - Video processing
  - `pydantic` - Data validation
  - `asyncio` - Parallel processing

### State Management
The system uses LangGraph's state management approach:

```python
# Claim Extractor State
@dataclass
class ClaimExtractorState:
    input_text: str
    sentences: List[str]
    selected_sentences: List[Sentence]
    disambiguated_claims: List[Claim]
    decomposed_claims: List[Claim]
    validated_claims: List[ValidatedClaim]
    metadata: Dict[str, Any]
```

```python
# Claim Verifier State
@dataclass
class ClaimVerifierState:
    claim: str
    search_queries: List[str]
    evidence: List[Evidence]
    search_iterations: int
    max_iterations: int
    verdict: Verdict
    reasoning: str
```

### Parallel Processing
Claims are verified in parallel with rate limiting:

```python
async def verify_claims_parallel(claims, max_concurrent=4):
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def verify_with_limit(claim):
        async with semaphore:
            return await verify_claim(claim)
    
    tasks = [verify_with_limit(claim) for claim in claims]
    return await asyncio.gather(*tasks)
```

### Cost Tracking
Real-time metrics tracked via `MetricsTracker`:

```python
tracker = get_metrics_tracker()

# Track LLM usage
tracker.record_llm_call(
    model="gpt-4o-mini",
    prompt_tokens=150,
    completion_tokens=50,
    total_tokens=200
)

# Track Tavily searches
tracker.record_tavily_call(num_results=10)

# Get summary
summary = tracker.get_summary()
# Returns: {
#     "llm_calls": 25,
#     "total_tokens": 5000,
#     "estimated_cost": 0.15,
#     "tavily_calls": 8
# }
```

### Configuration
Each agent has customizable settings:

**Claim Extractor** (`claim_extractor/llm/config.py`):
```python
LLM_CONFIG = {
    "model": "gpt-4o-mini",
    "temperature": 0.0,  # Deterministic
    "max_tokens": 2000
}
```

**Claim Verifier** (`claim_verifier/llm/config.py`):
```python
VERIFIER_CONFIG = {
    "model": "gpt-4o-mini",
    "temperature": 0.1,
    "max_search_iterations": 3,
    "max_results_per_search": 10,
    "timeout_seconds": 60
}
```

### Excel Report Structure

Generated reports include 4 sheets:

1. **Summary** - Accuracy metrics and cost breakdown
   - Overall accuracy percentage
   - Verdict distribution (Supported/Refuted/Not Enough Info)
   - Total OpenAI/Tavily costs
   - Generation timestamp

2. **Extracted Facts** - All claims found
   - Sequential numbering (#1, #2, etc.)
   - One claim per row
   - Clean, readable format

3. **Fact-Check Results** - Detailed verdicts
   - Original claim text
   - Corrected/clarified version
   - Verdict (color-coded)
   - Detailed reasoning
   - Search queries used

4. **Evidence Details** - Source information
   - Claim reference
   - Evidence snippets
   - Clickable source URLs
   - Website names

**Formatting Features**:
- Color-coded verdicts (green=supported, red=refuted, yellow=insufficient)
- Alternating row colors for readability
- Bold headers with background colors
- Auto-sized columns
- Hyperlinked URLs
- Professional borders and styling

## ï¿½ A Bit About the Research

The `claim_extractor` is built on the **Claimify** methodology from Metropolitansky & Larson's 2025 paper. It's pretty fascinating stuff - they figured out how to handle ambiguity and extract verifiable claims. I spent a good week just implementing their pipeline, and it was worth it. The full citation and details are in the [`claim_extractor/README.md`](./apps/agent/claim_extractor/README.md).

For the `claim_verifier`, the evidence retrieval approach draws some inspiration from the Search-Augmented Factuality Evaluator (SAFE) methodology in ["Long-form factuality in large language models"](https://arxiv.org/abs/2403.18802) by Wei et al. (2024). Just the basic idea of using search results to verify individual claims.

## âš ï¸ A Quick Note on the Implementation

Look, I've tried my best to faithfully implement everything described in the research papers, especially Claimify. But let's be real - there's always room for improvement and I might have missed some minor details along the way. I also took some creative liberties to enhance what was in the papers, adding features like the voting mechanism for disambiguation and the multi-retry approach for verification.

What you're seeing here is my interpretation of these research methods, with some practical additions that I found helpful when implementing in the real world. If you spot something that doesn't align perfectly with the papers, that's probably intentional - I was aiming for a working system that captured the spirit of the research while being practically useful.

The beauty of building on research is that we get to stand on the shoulders of giants AND add our own twist. I believe this implementation represents the core ideas faithfully while adding practical enhancements that make it even more effective.

## ğŸ§ª Development Setup

### Running from Source

1. **Clone and navigate**
```bash
git clone https://github.com/bharathxd/ClaimAI.git
cd ClaimAI/apps/streamlit
```

2. **Create virtual environment** (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure API keys** (choose one method)

**Method A: Streamlit Secrets** (recommended)
```bash
mkdir .streamlit
echo 'OPENAI_API_KEY = "sk-..."' > .streamlit/secrets.toml
echo 'TAVILY_API_KEY = "tvly-..."' >> .streamlit/secrets.toml
```

**Method B: Environment Variables**
```bash
export OPENAI_API_KEY="sk-..."
export TAVILY_API_KEY="tvly-..."
```

5. **Launch**
```bash
streamlit run standalone_app.py
```

### Desktop App

The desktop version uses Electron + Python backend:

```bash
cd apps/desktop

# Windows
START.bat

# Unix/Mac
chmod +x START.sh
./START.sh
```

### Running Individual Agents

Test agents independently:

```bash
cd apps/agent

# Extract claims only
python scripts/run_claim_extractor.py "Your text here"

# Verify single claim
python scripts/run_claim_verifier.py "Claim to verify"

# Full pipeline
python scripts/run_fact_checker.py "Complete text to analyze"
```

### Component Documentation

For detailed information on each agent:
* **[Claim Extractor](./apps/agent/claim_extractor/README.md)** - Claimify methodology implementation
* **[Claim Verifier](./apps/agent/claim_verifier/README.md)** - Evidence retrieval and evaluation
* **[Fact Checker](./apps/agent/fact_checker/README.md)** - Pipeline orchestration


## ğŸ“‚ Repository Structure

```
ClaimeAI-main/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ agent/                    # Core fact-checking agents
â”‚   â”‚   â”œâ”€â”€ claim_extractor/      # Agent 1: Extract claims from text
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py          # Main orchestration logic
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py        # LLM prompts for each stage
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py        # Pydantic models for state
â”‚   â”‚   â”‚   â”œâ”€â”€ config/           # Configuration
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ nodes.py      # Node definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/              # LLM configuration
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.py     # Model settings
â”‚   â”‚   â”‚   â””â”€â”€ nodes/            # Processing nodes
â”‚   â”‚   â”‚       â”œâ”€â”€ sentence_splitter.py
â”‚   â”‚   â”‚       â”œâ”€â”€ selection.py
â”‚   â”‚   â”‚       â”œâ”€â”€ disambiguation.py
â”‚   â”‚   â”‚       â”œâ”€â”€ decomposition.py
â”‚   â”‚   â”‚       â”œâ”€â”€ contextualization.py
â”‚   â”‚   â”‚       â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”‚       â””â”€â”€ validation.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ claim_verifier/       # Agent 2: Verify individual claims
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py          # Verification orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py        # Verification prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py        # State models
â”‚   â”‚   â”‚   â”œâ”€â”€ insufficient_info_analyzer.py  # Analyze missing info
â”‚   â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ nodes.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ nodes/
â”‚   â”‚   â”‚       â”œâ”€â”€ generate_search_query.py
â”‚   â”‚   â”‚       â”œâ”€â”€ retrieve_evidence.py
â”‚   â”‚   â”‚       â”œâ”€â”€ search_decision.py
â”‚   â”‚   â”‚       â””â”€â”€ evaluate_evidence.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ fact_checker/         # Agent 3: Orchestrate full pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py          # Main orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py        # Pipeline state models
â”‚   â”‚   â”‚   â””â”€â”€ nodes/
â”‚   â”‚   â”‚       â”œâ”€â”€ claim_verifier.py
â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/                # Shared utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ callbacks.py      # LangGraph callbacks
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py            # LLM helper functions
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.py        # Cost & usage tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py         # Shared data models
â”‚   â”‚   â”‚   â”œâ”€â”€ rate_limiter.py   # Concurrency control
â”‚   â”‚   â”‚   â”œâ”€â”€ redis.py          # Redis utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.py       # Configuration settings
â”‚   â”‚   â”‚   â””â”€â”€ text.py           # Text processing helpers
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ security/             # API key management
â”‚   â”‚   â”‚   â”œâ”€â”€ api_keys.py       # Key storage
â”‚   â”‚   â”‚   â””â”€â”€ auth.py           # Authentication
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ scripts/              # Utility scripts
â”‚   â”‚       â”œâ”€â”€ run_claim_extractor.py
â”‚   â”‚       â”œâ”€â”€ run_claim_verifier.py
â”‚   â”‚       â”œâ”€â”€ run_fact_checker.py
â”‚   â”‚       â””â”€â”€ dev.py
â”‚   â”‚
â”‚   â”œâ”€â”€ streamlit/                # Streamlit web interface
â”‚   â”‚   â”œâ”€â”€ standalone_app.py     # Main application (1,446 lines)
â”‚   â”‚   â”œâ”€â”€ export_excel.py       # Excel report generator (431 lines)
â”‚   â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”‚   â””â”€â”€ requirements_standalone.txt
â”‚   â”‚
â”‚   â””â”€â”€ desktop/                  # Desktop application (Electron)
â”‚       â”œâ”€â”€ electron/             # Electron frontend
â”‚       â”‚   â”œâ”€â”€ main.js           # Electron main process
â”‚       â”‚   â”œâ”€â”€ preload.js        # Preload script
â”‚       â”‚   â”œâ”€â”€ package.json
â”‚       â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ python-backend/       # Python backend server
â”‚       â”‚   â”œâ”€â”€ server.py
â”‚       â”‚   â”œâ”€â”€ config.ini
â”‚       â”‚   â””â”€â”€ requirements.txt
â”‚       â”œâ”€â”€ START.bat             # Windows launcher
â”‚       â”œâ”€â”€ START.sh              # Unix launcher
â”‚       â”œâ”€â”€ START_BACKEND.bat
â”‚       â””â”€â”€ START_ELECTRON.bat
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ README.md                 # This file
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # System architecture
â”‚   â””â”€â”€ CLAIM_EXTRACTOR_COST_ANALYSIS.md
â”‚
â””â”€â”€ pyproject.toml                # Project configuration
```

### Key Files

- **`apps/streamlit/standalone_app.py`** - Main Streamlit application with 4 modes
- **`apps/streamlit/export_excel.py`** - Professional Excel report generator
- **`apps/agent/claim_extractor/agent.py`** - Claimify-based extraction pipeline
- **`apps/agent/claim_verifier/agent.py`** - Evidence-based verification logic
- **`apps/agent/fact_checker/agent.py`** - Main orchestrator with parallel processing
- **`apps/agent/utils/metrics.py`** - Real-time cost and usage tracking

## ğŸ™ Thanks to the Giants

This project wouldn't have been possible without:

* Dasha Metropolitansky & Jonathan Larson from Microsoft Research - their Claimify methodology is brilliant
* Jerry Wei and team at Google DeepMind - their SAFE paper had some useful ideas for evidence retrieval
* The LangChain team - LangGraph made the complex workflows so much easier
* OpenAI - for the LLMs that power the text understanding
* Tavily AI - their search API is perfect for this use case

I've learned a ton working on this project. If you use it or have ideas for improvements, I'd love to hear about it! Contributions are always welcome - whether it's code, suggestions, or even just sharing how you're using it. Let's make this thing even better together.

## ğŸ›£ï¸ Roadmap

### Completed âœ…
- âœ… Multi-agent architecture with LangGraph
- âœ… 4 operation modes (Full Text, Single Fact, Claim Extraction, Video)
- âœ… Video fact-checking with Whisper transcription
- âœ… Multi-language support (100+ languages)
- âœ… Professional Excel reports with formatting
- âœ… Real-time cost tracking dashboard
- âœ… Parallel claim verification with rate limiting
- âœ… Authoritative source detection
- âœ… Desktop app (Electron + Python)
- âœ… Streamlit web interface

### In Progress ğŸ”„
- ğŸ”„ **Evaluation agent** - Assess fact-checking accuracy and reliability
- ğŸ”„ **Performance benchmarks** - Compare against human fact-checkers
- ğŸ”„ **Caching layer** - Redis-based result caching

### Planned ğŸ“‹
- ğŸ“‹ **Public API service** - RESTful API for external integrations
- ğŸ“‹ **Batch processing** - Handle multiple files/videos at once
- ğŸ“‹ **Custom model support** - Allow local LLMs (Ollama, LLaMA)
- ğŸ“‹ **Web scraping fallback** - Direct crawling when Tavily unavailable
- ğŸ“‹ **Citation graph** - Visualize evidence connections
- ğŸ“‹ **Real-time mode** - Process streaming text/audio
- ğŸ“‹ **Browser extension** - Fact-check while browsing
- ğŸ“‹ **Mobile app** - iOS/Android support

## ï¿½ Contributing

Contributions are welcome! Here's how you can help:

### Getting Started
1. **Fork** the repository
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes**
4. **Test thoroughly** - run all agents, check Excel export, verify metrics
5. **Commit**: `git commit -m 'Add amazing feature'`
6. **Push**: `git push origin feature/amazing-feature`
7. **Open a Pull Request**

### Contribution Guidelines
- âœ… **Code Style**: Follow existing patterns and PEP 8
- âœ… **Documentation**: Update README/docstrings for new features
- âœ… **Testing**: Add tests for new functionality
- âœ… **Dependencies**: Minimize new dependencies when possible
- âœ… **Compatibility**: Ensure Python 3.11+ compatibility

### Areas for Contribution
- ğŸ”§ **Performance**: Optimize LLM calls, reduce latency
- ğŸŒ **Languages**: Improve translation accuracy
- ğŸ“Š **Analytics**: Enhanced metrics and visualizations
- ğŸ§ª **Testing**: Unit tests, integration tests
- ğŸ“ **Documentation**: Tutorials, examples, API docs
- ğŸ› **Bug Fixes**: Always appreciated!

### Reporting Issues
Use the [GitHub Issue Tracker](https://github.com/bharathxd/ClaimAI/issues):
- ğŸ› **Bugs**: Include steps to reproduce, error messages, environment details
- ğŸ’¡ **Feature Requests**: Describe use case and expected behavior
- â“ **Questions**: Ask about usage, implementation, or architecture

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

## ï¿½ Contact & Support

- **Issues**: Please use the [GitHub Issue Tracker](https://github.com/bharathxd/agent/issues) to report bugs or request features
- **Email**: [bharathxxd@gmail.com](mailto:bharathxxd@gmail.com)
- **Twitter**: [@Bharath_uwu](https://twitter.com/bharath_uwu)
